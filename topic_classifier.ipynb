{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import lightning as ln\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as Func \n",
    "import pytorch_lightning as pl \n",
    "import torchmetrics\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  cyberbullying_type\n",
       "0      In other words #katandandre, your food was cra...                   3\n",
       "1      Why is #aussietv so white? #MKR #theblock #ImA...                   3\n",
       "2      @XochitlSuckkks a classy whore? Or more red ve...                   3\n",
       "3      @Jason_Gio meh. :P  thanks for the heads up, b...                   3\n",
       "4      @RudhoeEnglish This is an ISIS account pretend...                   3\n",
       "...                                                  ...                 ...\n",
       "47687  Black ppl aren't expected to do anything, depe...                   1\n",
       "47688  Turner did not withhold his disappointment. Tu...                   1\n",
       "47689  I swear to God. This dumb nigger bitch. I have...                   1\n",
       "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...                   1\n",
       "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...                   1\n",
       "\n",
       "[47692 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'cyberbullying_dataset.csv'\n",
    "pd_dataset = pd.read_csv(train_path)\n",
    "\n",
    "pd_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47692, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_ratio = 0.2\n",
    "\n",
    "# Calculate row indices for each split\n",
    "# split_index = int(len(pd_dataset) * split_ratio)\n",
    "\n",
    "# # Split the DataFrame into two parts\n",
    "# df1 = pd_dataset.iloc[:split_index, :]  # First half\n",
    "# df2 = pd_dataset.iloc[split_index:, :]  # Second half\n",
    "\n",
    "# # Save as CSV files\n",
    "# df1.to_csv('train.csv', index=False)\n",
    "# df2.to_csv('valid.csv', index=False)\n",
    "\n",
    "# print(\"DataFrames split and saved as CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyberbullying_type\n",
      "5    7998\n",
      "0    7992\n",
      "2    7973\n",
      "1    7961\n",
      "3    7945\n",
      "4    7823\n",
      "Name: count, dtype: int64\n",
      "####################\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47692 entries, 0 to 47691\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   tweet_text          47692 non-null  object\n",
      " 1   cyberbullying_type  47692 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 745.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pd_dataset['cyberbullying_type'].value_counts())\n",
    "print('#' * 20)\n",
    "print(pd_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n",
      "\n",
      "Index(['tweet_text', 'cyberbullying_type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "pd_dataset['cyberbullying_type'] = le.fit_transform(pd_dataset['cyberbullying_type'])\n",
    "classes  = le.classes_\n",
    "\n",
    "print(classes)\n",
    "print('')\n",
    "print(pd_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.495681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.709858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cyberbullying_type\n",
       "count        47692.000000\n",
       "mean             2.495681\n",
       "std              1.709858\n",
       "min              0.000000\n",
       "25%              1.000000\n",
       "50%              2.000000\n",
       "75%              4.000000\n",
       "max              5.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  cyberbullying_type\n",
      "0  In other words #katandandre, your food was cra...                   3\n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...                   3\n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...                   3\n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...                   3\n",
      "4  @RudhoeEnglish This is an ISIS account pretend...                   3\n"
     ]
    }
   ],
   "source": [
    "print(pd_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Accessing the data\n",
    "\n",
    "# # class TweetDataset(pl.LightningDataModule):\n",
    "# #     def __init__(self, csv_file, batch_size=32, val_size=0.2, test_size=0.2, random_state=42, transform=None):\n",
    "# #         super().__init__()\n",
    "# #         self.csv_file = csv_file\n",
    "# #         self.batch_size = batch_size\n",
    "# #         self.tokenizer = get_tokenizer('basic_english')\n",
    "# #         self.vocab = None\n",
    "# #         self.input_size = None\n",
    "# #         self.val_size = val_size\n",
    "# #         self.test_size = test_size\n",
    "# #         self.random_state = random_state\n",
    "# #         self.transform = None\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         data = pd.read_csv(self.csv_file)\n",
    "\n",
    "#         self.train_data, self.test_data = train_test_split(data, test_size=self.val_size + self.test_size, random_state=self.random_state)\n",
    "#         self.val_data, self.test_data = train_test_split(self.test_data, test_size=self.test_size, random_state=self.random_state)\n",
    "\n",
    "#         # self.train_data, self.val_data, self.test_data = train_test_split(data, test_size=self.val_size + self.test_size, random_state=self.random_state)\n",
    "#         # print(len(self.train_data), len(self.val_data), len(self.test_data))\n",
    "\n",
    "#         text_iterator = (text for text in self.train_data['tweet_text'])\n",
    "#         self.vocab = build_vocab_from_iterator(text_iterator, specials=['<UNK>'])\n",
    "#         self.vocab.set_default_index(self.vocab['<UNK>'])\n",
    "\n",
    "#         self.input_size = len(self.vocab)\n",
    "    \n",
    "\n",
    "#     def __len__(self):\n",
    "#         if self.train_data is not None:\n",
    "#             return len(self.train_data)\n",
    "#         elif self.val_data is not None:\n",
    "#             return len(self.val_data)\n",
    "#         elif self.test_data is not None:\n",
    "#             return len(self.test_data)\n",
    "#         else:\n",
    "#             return 0\n",
    "\n",
    "\n",
    "        \n",
    "#     # def __len__(self):\n",
    "#     #     return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         if torch.is_tensor(index):\n",
    "#             index = index.tolist()\n",
    "\n",
    "#         if self.train_data is not None:\n",
    "#             sample = self.train_data.iloc[index]\n",
    "#         elif self.val_data is not None:\n",
    "#             sample = self.val_data.iloc[index]\n",
    "#         elif self.test_data is not None:\n",
    "#             sample = self.test_data.iloc[index]\n",
    "#         else:\n",
    "#             raise ValueError(\"No data has been set up for the dataset.\")\n",
    "\n",
    "#         if self.transform:\n",
    "#             sample = self.transform(sample)\n",
    "\n",
    "#         return sample\n",
    "    \n",
    "#     def collate_fn(self, batch_idx):\n",
    "#         text = [self.vocab(self.tokenizer(row['tweet_text'])) for row in batch_idx]\n",
    "#         label = [row['cyberbullying_type'] for row in batch_idx]\n",
    "       \n",
    "#         return torch.tensor(text), torch.tensor(label)\n",
    "\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn, num_workers=3)\n",
    "    \n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=3)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=3)\n",
    "\n",
    "#     # def collate_fn(self, batch):\n",
    "#     #     text = [self.vocab(self.tokenizer(row['tweet_text'])) for row in batch]\n",
    "#     #     label = [row['cyberbullying_type'] for row in batch]\n",
    "\n",
    "#     #     return torch.tensor(text), torch.tensor(label)\n",
    "\n",
    "    \n",
    "# def my_collate_fn(batch):\n",
    "#     try:\n",
    "#         return [self.dataset[idx] for idx in batch]\n",
    "#     except KeyError as e:\n",
    "#         print(f\"KeyError occurred in DataLoader worker process: {e}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(pl.LightningDataModule):\n",
    "    def __init__(self, csv_file, batch_size=32, random_state=42):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        # self.val_size = val_size\n",
    "        # self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "        self.vocab = None\n",
    "        self.input_size = None\n",
    "        self.train_data = None\n",
    "        # self.val_data = None\n",
    "        # self.test_data = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        text = [self.vocab(self.tokenizer(row['tweet_text'])) for row in batch]\n",
    "        label = [row['cyberbullying_type'] for row in batch]\n",
    "        return torch.tensor(text), torch.tensor(label)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_data = pd.read_csv(self.csv_file)\n",
    "        # self.train_data, temp_data = train_test_split(data, test_size=self.val_size + self.test_size, random_state=self.random_state)\n",
    "        # self.val_data, self.test_data = train_test_split(temp_data, test_size=self.test_size, random_state=self.random_state)\n",
    "        self.train_data.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        text_iterator = (text for text in self.train_data['tweet_text'])\n",
    "        self.vocab = build_vocab_from_iterator(text_iterator, specials=['<UNK>'])\n",
    "        self.vocab.set_default_index(self.vocab['<UNK>'])\n",
    "        self.input_size = len(self.vocab)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn, num_workers=3)\n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     return DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=3)\n",
    "\n",
    "    # def test_dataloader(self):\n",
    "    #     return DataLoader(self.test_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn, num_workers=3)\n",
    "   \n",
    "    def __len__(self):\n",
    "        if self.train_data is not None:\n",
    "            return len(self.train_data)\n",
    "        # elif self.val_data is not None:\n",
    "        #     return len(self.val_data)\n",
    "        # elif self.test_data is not None:\n",
    "        #     return len(self.test_data)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47692\n"
     ]
    }
   ],
   "source": [
    "dataset = TweetDataset('cyberbullying_dataset.csv')\n",
    "\n",
    "dataset.setup()\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dataset:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataloader = DataLoader(dataset, batch_size=32, collate_fn=my_collate_fn)\n",
    "\n",
    "# train_dataloader = DataLoader(dataset, batch_size=32, collate_fn=my_collate_fn)\n",
    "# train_dataset = dataset.train_dataloader()\n",
    "# val_dataset = dataset.val_dataloader()\n",
    "\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(pl.LightningModule):\n",
    "    def __init__(self, input_size, num_classes=6, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.embedding = None\n",
    "\n",
    "        pretrained_embeddings = torch.randn(input_size, 64)\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        # self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        # self.test_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_id):\n",
    "        x, y = batch\n",
    "        logit = self(x)\n",
    "        loss = Func.cross_entropy(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc)\n",
    "        return loss \n",
    "\n",
    "    # def validation_step(self, batch, batch_id):\n",
    "    #     x, y = batch\n",
    "    #     logits = self(x)\n",
    "    #     loss = F.cross_entropy(logits, y)\n",
    "    #     preds = logits.argmax(dim=1)\n",
    "    #     self.val_acc(preds, y)\n",
    "    #     self.log('val_loss', loss)\n",
    "    #     self.log('val_acc', self.val_acc)\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     x, y = batch\n",
    "    #     logits = self(x)\n",
    "    #     loss = F.cross_entropy(logits, y)\n",
    "    #     preds = logits.argmax(dim=1)\n",
    "    #     self.test_acc(preds, y)\n",
    "    #     self.log('test_loss', loss)\n",
    "    #     self.log('test_acc', self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(807, 64)\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (train_acc): MulticlassAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model = TextClassifier(input_size=dataset.input_size, num_classes=6)\n",
    "\n",
    "detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | embedding | Embedding          | 51.6 K\n",
      "1 | fc1       | Linear             | 4.2 K \n",
      "2 | fc2       | Linear             | 390   \n",
      "3 | relu      | ReLU               | 0     \n",
      "4 | train_acc | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "56.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "56.2 K    Total params\n",
      "0.225     Total estimated model params size (MB)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7efd14e02950>Exception ignored in: \n",
      "Traceback (most recent call last):\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7efd14e02950>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "\n",
      "      File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1462, in _shutdown_workers\n",
      "if w.is_alive():"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362a62d462c1469dab09d6eaa429ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "if w.is_alive():"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError occurred: Caught KeyError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3791, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 17219\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/frame.py\", line 3893, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3798, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 17219\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AssertionError\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
      "\n",
      "AssertionError: can only test a child process\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    trainer.fit(detection_model, dataset)\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
